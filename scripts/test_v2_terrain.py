#!/usr/bin/env python3
"""Test terrain minimal pour Titan Scraper v2.

Ce script effectue un test R√âEL avec LinkedIn mais avec des quotas tr√®s bas
pour valider le flux complet v2 sans risquer de sur-extraction.

IMPORTANT: N√©cessite une session LinkedIn valide (storage_state.json)

Usage:
    # Mode dry-run (simulation sans vraie requ√™te LinkedIn)
    python scripts/test_v2_terrain.py --dry-run

    # Test r√©el avec 2 posts max
    python scripts/test_v2_terrain.py --quota 2

    # Test r√©el avec entreprise sp√©cifique
    python scripts/test_v2_terrain.py --company "Bredin Prat" --quota 2
"""
from __future__ import annotations

import argparse
import json
import os
import sys
import tempfile
from datetime import datetime
from pathlib import Path

# Ensure project root is in path
PROJECT_ROOT = Path(__file__).resolve().parent.parent
sys.path.insert(0, str(PROJECT_ROOT))

# Force v2 mode
os.environ["TITAN_ENABLE_V2"] = "1"


def print_header(title: str):
    """Print a formatted header."""
    print("\n" + "=" * 60)
    print(f" {title}")
    print("=" * 60)


def print_section(title: str):
    """Print a section header."""
    print(f"\n--- {title} ---")


def test_prequal_with_real_samples():
    """Test pre-qualification with realistic LinkedIn post samples."""
    print_section("Test pr√©-qualification avec √©chantillons r√©alistes")
    
    from scraper.pre_qualifier import pre_qualify_post, PreQualificationMetrics
    
    # Realistic test samples
    samples = [
        # Should ACCEPT
        {
            "text": "üöÄ Nous recrutons un Juriste M&A (H/F) pour rejoindre notre √©quipe √† Paris. CDI. Exp√©rience 3-5 ans en droit des soci√©t√©s. Candidature: recrutement@cabinet.fr",
            "author": "Cabinet Bredin Prat",
            "expected": True,
            "reason": "Recrutement juridique direct",
        },
        {
            "text": "Notre cabinet recherche un avocat collaborateur en droit social. Poste bas√© √† Lyon. Rejoignez une √©quipe dynamique!",
            "author": "Fidal Lyon",
            "expected": True,
            "reason": "Avocat droit social",
        },
        {
            "text": "Offre d'emploi: Directeur Juridique Groupe. Mission: piloter la strat√©gie juridique, M&A, compliance. Paris La D√©fense.",
            "author": "LVMH",
            "expected": True,
            "reason": "Direction juridique corporate",
        },
        # Should REJECT
        {
            "text": "üì¢ Stage de fin d'√©tudes - Juriste droit des affaires. 6 mois √† partir de septembre. Gratification l√©gale.",
            "author": "Some Company",
            "expected": False,
            "reason": "Stage (exclusion)",
        },
        {
            "text": "Nous recrutons pour notre client, un grand cabinet d'avocats, un associ√© corporate M&A.",
            "author": "Michael Page Legal",
            "expected": False,
            "reason": "Agence de recrutement",
        },
        {
            "text": "Alternance droit des contrats - 24 mois. Vous pr√©parez un Master 2 en droit des affaires.",
            "author": "BNP Paribas",
            "expected": False,
            "reason": "Alternance (exclusion)",
        },
        {
            "text": "üéâ Bienvenue √† Marie qui rejoint notre √©quipe juridique! Nous sommes ravis de l'accueillir.",
            "author": "Soci√©t√© G√©n√©rale",
            "expected": False,
            "reason": "Non-recrutement (welcome post)",
        },
        {
            "text": "Seeking a Senior Legal Counsel for our London office. 7+ years experience in M&A required.",
            "author": "Clifford Chance",
            "expected": False,
            "reason": "Poste √† l'√©tranger (London)",
        },
        {
            "text": "#OpenToWork Je recherche un poste de juriste contrats. 5 ans d'exp√©rience. Disponible imm√©diatement.",
            "author": "Jean Dupont",
            "expected": False,
            "reason": "Demandeur d'emploi",
        },
    ]
    
    metrics = PreQualificationMetrics()
    passed = 0
    failed = 0
    
    for sample in samples:
        result = pre_qualify_post(
            preview_text=sample["text"][:300],
            author_name=sample["author"],
        )
        metrics.record(result)
        
        correct = result.should_extract == sample["expected"]
        status = "‚úì" if correct else "‚úó"
        
        if correct:
            passed += 1
        else:
            failed += 1
        
        expected_str = "ACCEPT" if sample["expected"] else "REJECT"
        actual_str = "ACCEPT" if result.should_extract else "REJECT"
        
        print(f"  {status} [{expected_str}‚Üí{actual_str}] {sample['author'][:25]:<25} | {sample['reason']}")
        if not correct:
            print(f"      D√©tail: {result.reason}")
    
    print(f"\nR√©sultats: {passed}/{len(samples)} corrects")
    
    stats = metrics.to_dict()
    print(f"\nM√©triques pr√©-qualification:")
    print(f"  - Total v√©rifi√©: {stats['total_checked']}")
    print(f"  - Accept√©s: {stats['accepted']}")
    print(f"  - Rejet√©s: {stats['total_checked'] - stats['accepted']}")
    print(f"  - Taux de rejet: {stats['rejection_rate']:.1%}")
    print(f"  - √âconomie estim√©e: {stats['savings_estimate']:.1%}")
    
    return passed == len(samples)


def test_whitelist_for_session():
    """Test que la whitelist retourne des entreprises pour une session."""
    print_section("Test whitelist pour session")
    
    from scraper.company_whitelist import get_company_whitelist
    
    whitelist = get_company_whitelist()
    
    # Test diff√©rents types de sessions
    session_types = ["tier1_check", "tier2_check", "exploration", "default"]
    
    for session_type in session_types:
        companies = whitelist.get_companies_for_session(session_type, max_companies=3)
        print(f"  {session_type}: {len(companies)} entreprises")
        for c in companies[:2]:
            print(f"    - {c.name} (Tier {c.tier})")
    
    stats = whitelist.get_stats()
    print(f"\nStats whitelist:")
    print(f"  - Total: {stats['total_companies']}")
    print(f"  - √Ä visiter: {stats['due_for_visit']}")
    
    return True


def test_session_orchestrator_status():
    """Test l'√©tat actuel du session orchestrator."""
    print_section("√âtat du Session Orchestrator")
    
    from scraper.session_orchestrator import get_session_orchestrator
    
    orchestrator = get_session_orchestrator()
    
    can_scrape, reason = orchestrator.should_scrape_now()
    print(f"  Peut scraper maintenant: {can_scrape} ({reason})")
    
    stats = orchestrator.get_daily_stats()
    print(f"  Quota journalier: {stats['quota_target']}")
    print(f"  Posts qualifi√©s aujourd'hui: {stats['posts_qualified']}")
    print(f"  Sessions compl√©t√©es: {stats['sessions_completed']}")
    
    wait = orchestrator.get_wait_seconds()
    print(f"  Attente prochaine session: {wait}s ({wait // 60}m)")
    
    quota = orchestrator.get_session_quota()
    print(f"  Quota session actuelle: {quota}")
    
    return True


def simulate_scrape_flow(quota: int = 3, dry_run: bool = True):
    """Simulate le flux de scraping complet avec pr√©-qualification.
    
    Args:
        quota: Nombre max de posts √† accepter
        dry_run: Si True, simule sans vraie requ√™te LinkedIn
    """
    print_section(f"Simulation flux scraping (quota={quota}, dry_run={dry_run})")
    
    from scraper.pre_qualifier import pre_qualify_post, PreQualificationMetrics
    from scraper.session_orchestrator import get_session_orchestrator
    from scraper.company_whitelist import get_company_whitelist
    
    # Simulated posts that would come from LinkedIn
    simulated_raw_posts = [
        {"author": "Michael Page Legal", "text": "Notre client, un cabinet leader, recrute...", "preview": "Notre client recrute"},
        {"author": "Bredin Prat", "text": "Nous recherchons un collaborateur corporate M&A pour notre bureau de Paris...", "preview": "Nous recherchons un collaborateur"},
        {"author": "Random Person", "text": "Stage 6 mois en droit bancaire √† partir de mars...", "preview": "Stage 6 mois"},
        {"author": "Gide Loyrette", "text": "Offre CDI: Juriste droit des soci√©t√©s exp√©riment√©. 5 ans minimum...", "preview": "Offre CDI Juriste"},
        {"author": "Jean Martin", "text": "#OpenToWork Juriste disponible imm√©diatement...", "preview": "#OpenToWork"},
        {"author": "Clifford Chance Paris", "text": "Recrutement: Avocat fiscaliste senior pour notre √©quipe M&A √† Paris...", "preview": "Recrutement Avocat"},
        {"author": "LVMH Legal", "text": "Direction juridique: poste de Responsable Contrats Groupe...", "preview": "Direction juridique"},
        {"author": "Hays Legal", "text": "Urgent: notre client recherche un directeur juridique...", "preview": "notre client recherche"},
    ]
    
    metrics = PreQualificationMetrics()
    accepted_posts = []
    rejected_posts = []
    
    print(f"\n  Traitement de {len(simulated_raw_posts)} posts simul√©s...")
    print()
    
    for i, post in enumerate(simulated_raw_posts, 1):
        # Phase 1: Pr√©-qualification
        result = pre_qualify_post(
            preview_text=post["preview"],
            author_name=post["author"],
        )
        metrics.record(result)
        
        if result.should_extract:
            if len(accepted_posts) < quota:
                accepted_posts.append(post)
                print(f"  [{i}] ‚úì ACCEPT: {post['author'][:30]} (conf: {result.confidence:.2f})")
                
                # Early exit si quota atteint
                if len(accepted_posts) >= quota:
                    print(f"\n  ‚ö° QUOTA ATTEINT ({quota} posts) - arr√™t anticip√©")
                    break
            else:
                print(f"  [{i}] ‚äò QUOTA FULL: {post['author'][:30]}")
        else:
            rejected_posts.append((post, result.reason))
            print(f"  [{i}] ‚úó REJECT: {post['author'][:30]} ‚Üí {result.reason}")
    
    print(f"\n  R√©sum√©:")
    print(f"    - Posts trait√©s: {metrics.to_dict()['total_checked']}")
    print(f"    - Accept√©s: {len(accepted_posts)}")
    print(f"    - Rejet√©s par pr√©-qual: {len(rejected_posts)}")
    print(f"    - Quota utilis√©: {len(accepted_posts)}/{quota}")
    
    if rejected_posts:
        print(f"\n  D√©tail rejets:")
        reasons = {}
        for post, reason in rejected_posts:
            reasons[reason] = reasons.get(reason, 0) + 1
        for reason, count in sorted(reasons.items(), key=lambda x: -x[1]):
            print(f"    - {reason}: {count}")
    
    return len(accepted_posts) <= quota


def run_real_scrape_test(quota: int = 2, keyword: str = "juriste recrutement"):
    """Lance un vrai test de scraping avec quota minimal.
    
    ATTENTION: Ceci fait une vraie requ√™te LinkedIn!
    """
    print_section(f"TEST R√âEL LinkedIn (quota={quota})")
    
    storage_state_path = PROJECT_ROOT / "storage_state.json"
    
    if not storage_state_path.exists():
        print("  ‚ùå ERREUR: storage_state.json introuvable")
        print("     Vous devez d'abord vous connecter √† LinkedIn via l'interface desktop.")
        return False
    
    print(f"  ‚úì Session LinkedIn trouv√©e: {storage_state_path}")
    print(f"  Keyword: {keyword}")
    print(f"  Quota: {quota} posts max")
    
    # Pr√©parer l'input pour scrape_subprocess
    input_data = {
        "keywords": [keyword],
        "storage_state": str(storage_state_path),
        "max_per_keyword": 10,
        "headless": True,
        "session_quota": quota,  # v2: Limite de session
    }
    
    print(f"\n  ‚è≥ Lancement du scraping (cela peut prendre 1-2 minutes)...")
    
    import subprocess
    import json
    
    # Cr√©er fichiers temporaires pour I/O
    with tempfile.NamedTemporaryFile(mode='w', suffix='.json', delete=False) as f_in:
        json.dump(input_data, f_in)
        input_file = f_in.name
    
    output_file = tempfile.mktemp(suffix='.json')
    
    try:
        # Lancer le subprocess
        cmd = [
            sys.executable,
            str(PROJECT_ROOT / "scraper" / "scrape_subprocess.py"),
            "--input-file", input_file,
            "--output-file", output_file,
        ]
        
        result = subprocess.run(
            cmd,
            capture_output=True,
            text=True,
            timeout=180,  # 3 minutes max
            cwd=str(PROJECT_ROOT),
        )
        
        if result.returncode != 0:
            print(f"  ‚ùå Erreur subprocess: {result.stderr[:500]}")
            return False
        
        # Lire les r√©sultats
        with open(output_file, 'r', encoding='utf-8') as f:
            scrape_result = json.load(f)
        
        print(f"\n  R√©sultats:")
        print(f"    - Succ√®s: {scrape_result.get('success', False)}")
        print(f"    - Posts extraits: {len(scrape_result.get('posts', []))}")
        print(f"    - Quota atteint: {scrape_result.get('session_quota_reached', False)}")
        
        stats = scrape_result.get('stats', {})
        if stats:
            print(f"\n  Statistiques:")
            print(f"    - Total scrap√©: {stats.get('total_scraped', 0)}")
            print(f"    - Accept√©s: {stats.get('accepted', 0)}")
            print(f"    - Rejet√©s agence: {stats.get('rejected_agency', 0)}")
            print(f"    - Rejet√©s externe: {stats.get('rejected_external', 0)}")
            print(f"    - Rejet√©s stage: {stats.get('rejected_contract_type', 0)}")
            print(f"    - Rejet√©s non-fran√ßais: {stats.get('rejected_non_french', 0)}")
            print(f"    - Rejet√©s duplicate: {stats.get('rejected_duplicate', 0)}")
        
        if scrape_result.get('posts'):
            print(f"\n  Posts accept√©s:")
            for i, post in enumerate(scrape_result['posts'][:5], 1):
                author = post.get('author', 'Unknown')[:30]
                text_preview = post.get('text', '')[:60]
                print(f"    {i}. {author}: {text_preview}...")
        
        if scrape_result.get('errors'):
            print(f"\n  Erreurs:")
            for err in scrape_result['errors'][:3]:
                print(f"    - {err[:100]}")
        
        return scrape_result.get('success', False)
        
    except subprocess.TimeoutExpired:
        print("  ‚ùå Timeout (>3 minutes)")
        return False
    except Exception as e:
        print(f"  ‚ùå Erreur: {e}")
        return False
    finally:
        # Cleanup
        if os.path.exists(input_file):
            os.unlink(input_file)
        if os.path.exists(output_file):
            os.unlink(output_file)


def main():
    parser = argparse.ArgumentParser(
        description="Test terrain minimal pour Titan Scraper v2"
    )
    parser.add_argument(
        "--dry-run",
        action="store_true",
        help="Mode simulation sans vraie requ√™te LinkedIn"
    )
    parser.add_argument(
        "--quota",
        type=int,
        default=2,
        help="Nombre max de posts √† accepter (d√©faut: 2)"
    )
    parser.add_argument(
        "--keyword",
        type=str,
        default="juriste recrutement CDI",
        help="Mot-cl√© de recherche pour test r√©el"
    )
    parser.add_argument(
        "--real",
        action="store_true",
        help="Lancer un vrai test LinkedIn (ATTENTION: requ√™te r√©elle)"
    )
    
    args = parser.parse_args()
    
    print_header("TITAN SCRAPER V2 - TEST TERRAIN MINIMAL")
    print(f"Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"Mode: {'DRY-RUN (simulation)' if args.dry_run or not args.real else 'R√âEL'}")
    print(f"Quota: {args.quota} posts max")
    
    all_passed = True
    
    # Test 1: Pr√©-qualification avec √©chantillons
    try:
        if not test_prequal_with_real_samples():
            print("\n  ‚ö† Certains tests de pr√©-qualification ont √©chou√©")
            all_passed = False
    except Exception as e:
        print(f"\n  ‚ùå Erreur test pr√©-qualification: {e}")
        all_passed = False
    
    # Test 2: Whitelist
    try:
        test_whitelist_for_session()
    except Exception as e:
        print(f"\n  ‚ùå Erreur test whitelist: {e}")
        all_passed = False
    
    # Test 3: Session orchestrator
    try:
        test_session_orchestrator_status()
    except Exception as e:
        print(f"\n  ‚ùå Erreur test orchestrator: {e}")
        all_passed = False
    
    # Test 4: Simulation du flux
    try:
        if not simulate_scrape_flow(quota=args.quota, dry_run=True):
            all_passed = False
    except Exception as e:
        print(f"\n  ‚ùå Erreur simulation flux: {e}")
        all_passed = False
    
    # Test 5: Test r√©el si demand√©
    if args.real and not args.dry_run:
        print("\n" + "!" * 60)
        print(" ATTENTION: Lancement d'un test R√âEL LinkedIn")
        print(" Cela fera une vraie requ√™te avec votre compte.")
        print("!" * 60)
        
        confirm = input("\nConfirmer? (oui/non): ").strip().lower()
        if confirm == "oui":
            try:
                if not run_real_scrape_test(quota=args.quota, keyword=args.keyword):
                    all_passed = False
            except Exception as e:
                print(f"\n  ‚ùå Erreur test r√©el: {e}")
                all_passed = False
        else:
            print("  Test r√©el annul√©.")
    
    # R√©sum√© final
    print_header("R√âSUM√â")
    
    if all_passed:
        print("‚úÖ Tous les tests ont r√©ussi!")
        print("\nProchaines √©tapes recommand√©es:")
        print("  1. Lancer un test r√©el avec: python scripts/test_v2_terrain.py --real --quota 2")
        print("  2. V√©rifier les logs dans %LOCALAPPDATA%\\TitanScraper\\scrape_subprocess_debug.txt")
        print("  3. Si OK, lancer une session compl√®te avec TITAN_ENABLE_V2=1")
    else:
        print("‚ö† Certains tests ont √©chou√© - v√©rifiez les erreurs ci-dessus")
        return 1
    
    return 0


if __name__ == "__main__":
    sys.exit(main())
