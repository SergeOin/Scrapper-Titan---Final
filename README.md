# LinkedIn Scraper & Minimal Dashboard

[![CI](https://github.com/SergeOin/Scrapper-Titan---Final/actions/workflows/ci.yml/badge.svg)](https://github.com/SergeOin/Scrapper-Titan---Final/actions/workflows/ci.yml) [![Coverage](https://codecov.io/gh/SergeOin/Scrapper-Titan---Final/branch/main/graph/badge.svg?token=CODECOV_TOKEN_PLACEHOLDER)](https://codecov.io/gh/SergeOin/Scrapper-Titan---Final)

> (Si le badge coverage est gris, le token ou l‚Äôupload n‚Äôest pas encore configur√©; voir section Qualit√© pour fallback.)

> Usage interne uniquement. Respect strict des CGU LinkedIn. Ce projet fournit un worker de scraping d√©coupl√© d'un serveur FastAPI avec un mini dashboard pour visualiser les posts collect√©s et d√©clencher un nouveau scrape de fa√ßon contr√¥l√©e.

---
## üéØ Objectifs
- Scraping de posts LinkedIn √† partir de mots-cl√©s cibl√©s (recherche)
- Stockage principal MongoDB (Atlas), fallback automatique SQLite ou CSV
- Worker asynchrone (s√©par√© du serveur) + file/queue Redis pour jobs
- Dashboard FastAPI minimal (table pagin√©e + stats + bouton "Forcer scrape")
- Cache TTL & verrou anti-concurrence pour √©viter sur-scraping
- Logging structur√© JSON + rotation + m√©triques Prometheus `/metrics`
- Tests unitaires (pytest), linting Ruff, format Black, typage mypy
- Docker multi-stage pr√™t pour d√©ploiement serveur Linux (Playwright install√©)

### Extension Domaine Juridique (Classification)
Pipeline int√©gr√© de qualification des posts juridiques en France :
* Filtrage langue FR + heuristique g√©ographique France (d√©tection mots FR + mentions FR/Paris/provinces)
* Classifieur heuristique (`scraper/legal_classifier.py`) ‚áí intent `recherche_profil` vs `autre`
* Scoring `relevance_score` (0..1) + `confidence` + `keywords_matched` (liste d√©-dupliqu√©e)
* Limite dure quotidienne `LEGAL_DAILY_POST_CAP` (par d√©faut 50) ‚Äì m√©trique `legal_daily_cap_reached_total`
* Cap visible via endpoint `/api/legal_stats` + barre de progression sur le dashboard
* Filtrage API & UI: query param `?intent=recherche_profil|autre`
* Script de purge cibl√©e par intent: `python scripts/purge_intent.py --intent recherche_profil`
* Champs persist√©s (Mongo / SQLite colonnes d√©di√©es / CSV fallback enrichi) :
  - `intent`
  - `relevance_score`
  - `confidence`
  - `keywords_matched`
  - `location_ok`
* Nouvelles colonnes SQLite (migration auto best‚Äëeffort) : `intent`, `relevance_score`, `confidence`, `keywords_matched`, `location_ok`
* M√©triques Prometheus :
  - `legal_posts_total`
  - `legal_posts_discarded_total{reason="intent|location"}`
  - `legal_intent_classifications_total{intent}`
  - `legal_daily_cap_reached_total`
* Param√®tres environnement sp√©cifiques :
  | Variable | R√¥le | D√©faut |
  |----------|------|--------|
  | `LEGAL_DAILY_POST_CAP` | Nombre max de posts l√©gaux persist√©s / jour UTC | 50 |
  | `LEGAL_INTENT_THRESHOLD` | Seuil score combin√© pour passer en `recherche_profil` | 0.35 |
  | `LEGAL_KEYWORDS` | (Optionnel) liste ; s√©par√©e de mots-cl√©s m√©tiers pour override/extension | ‚Äî |
  | `FILTER_LEGAL_DOMAIN_ONLY` | Si `1`, force le worker √† ne garder que le domaine juridique (pr√©-filtrage) | 0 |
* Objet classification (exemple) :
```jsonc
{
  "intent": "recherche_profil",
  "relevance_score": 0.74,
  "confidence": 0.78,
  "keywords_matched": ["avocat", "juriste"],
  "location_ok": true
}
```
* Endpoint stats journali√®res :
```bash
GET /api/legal_stats
‚Üí {
  "date": "2025-10-03",
  "accepted": 31,
  "discarded_intent": 14,
  "discarded_location": 3,
  "discarded_total": 17,
  "total_classified": 48,
  "cap": 50,
  "cap_remaining": 19,
  "cap_progress": 0.62,
  "rejection_rate": 0.3542,
  "intent_threshold": 0.35
}
```
* Param√®tre debug `include_raw=1` (API `/api/posts`) pour exposer le bloc `classification_debug` (intent, scores, keywords) ‚Äì omis par d√©faut pour r√©duire la taille.
* Conformit√©: voir `COMPLIANCE.md` (minimisation, usage interne, absence de techniques de contournement)
* Suivi interne suppl√©mentaire (non persist√© dans les documents de posts mais expos√© via `/api/legal_stats`) :
  - `accepted` (posts retenus)
  - `discarded_intent` (intent != recherche_profil)
  - `discarded_location` (rejets localisation)
  - `cap_remaining`, `cap_progress` (quota)

---
## üß± Architecture (vue d'ensemble)
```
project/
‚îÇ-- scraper/
‚îÇ   ‚îÇ-- bootstrap.py      # Initialisation context: settings, clients, logging
‚îÇ   ‚îÇ-- utils.py          # Outils communs (UA random, parse date, langue, etc.)
‚îÇ   ‚îÇ-- worker.py         # Logique scraping + retries + stockage + screenshots
‚îÇ-- server/
‚îÇ   ‚îÇ-- main.py           # App FastAPI, montage routes, middlewares, metrics
‚îÇ   ‚îÇ-- routes.py         # Endpoints API + dashboard HTML
‚îÇ   ‚îÇ-- templates/
‚îÇ   ‚îÇ    ‚îî‚îÄ dashboard.html# UI unique minimaliste
‚îÇ-- scripts/
‚îÇ   ‚îÇ-- run_once.py       # Lance un job de scraping isol√© (sans queue)
‚îÇ   ‚îÇ-- start_server.ps1  # D√©marrage serveur (Windows)
‚îÇ   ‚îÇ-- run_scraper.ps1   # D√©marrage worker (Windows)
‚îÇ-- tests/                # Tests unitaires & snapshots s√©lecteurs
‚îÇ-- Dockerfile
‚îÇ-- requirements.txt
‚îÇ-- .env.example
‚îÇ-- README.md
‚îÇ-- .gitignore
```

---
## ‚öôÔ∏è Flux Fonctionnel
1. L'utilisateur (interne) ouvre le dashboard ‚áí voit les posts + stats (dernier run, total, √©tat queue)
2. Il clique sur "Forcer scrape" (POST) ‚áí push d'un job keyword(s) dans Redis
3. Le worker (process s√©par√©) consomme la queue ‚áí Playwright + login via `storage_state.json`
4. Le worker applique les s√©lecteurs (abstraction test√©e) ‚áí extrait posts (texte, auteur, date, langue, score heuristique)
5. Stockage MongoDB (ou fallback) + mise √† jour m√©tadonn√©es (last_run, counts)
6. Logs JSON + snapshots d'erreur (screenshots) + m√©triques increment√©es
7. Le dashboard affiche les nouvelles donn√©es via pagination / query params.

---
## üóÑÔ∏è Stockage
Ordre de priorit√© :
1. MongoDB (Motor + collection `posts` & `meta`) ‚Äì backend principal
2. SQLite (fichier local `fallback.sqlite3`) si Mongo indisponible
3. CSV (append dans `exports/fallback_posts.csv`) si SQLite √©choue

Sch√©ma persistant actuel (champs de score supprim√©s) :
```jsonc
{
  "_id": "hash(post_url|timestamp)",
  "keyword": "python ai",
  "author": "Nom Auteur",
  "author_profile": "https://www.linkedin.com/in/...",
  "company": "Entreprise XYZ", // si d√©tect√©e ou d√©riv√©e heuristiquement
  "text": "Contenu du post...",
  "language": "fr",
  "published_at": "2025-09-18T08:21:00Z",
  "collected_at": "2025-09-18T08:25:12Z",
  "permalink": "https://www.linkedin.com/feed/update/urn:li:activity:XXXX/",
  "raw": { /* fragments bruts pour debug */ }
}
```

---
## üîí S√©curit√© & Conformit√©
- Variables sensibles uniquement via `.env` (jamais commit) : credentials, URIs
- Aucune redistribution publique des donn√©es collect√©es
- Respect des limitations implicites (sleep jitter, random UA)
- Possibilit√© de d√©sactiver le scraping global via variable `SCRAPING_ENABLED=0`
- Option d'activer une auth basique interne (`INTERNAL_AUTH_USER/PASS`)
- Jeton de protection d√©clenchement job (`TRIGGER_TOKEN`) activ√© si d√©fini : envoyer le header `X-Trigger-Token: <valeur>` sur `POST /trigger`
- HTTPS g√©r√© en amont (reverse proxy) ‚Äî possibilit√© future d'ajouter TLS local

---
## üì¶ Variables d'environnement (voir `.env.example`)
| Variable | Description | Exemple |
|----------|-------------|---------|
| `MONGO_URI` | URI MongoDB Atlas | `mongodb+srv://user:pass@cluster/db` |
| `MONGO_DB` | Nom DB | `linkedin_scrape` |
| `REDIS_URL` | Redis queue/cache | `redis://localhost:6379/0` |
| `SCRAPE_KEYWORDS` | Liste mots-cl√©s (s√©par√©s par ;) | `python;ai;data` |
| `SCRAPING_ENABLED` | 1/0 activer d√©sactiver | `1` |
| `PLAYWRIGHT_HEADLESS` | Mode headless | `1` |
| `CACHE_TTL_SECONDS` | TTL cache en secondes | `300` |
| `LOCK_FILE` | Fichier de verrou | `.scrape.lock` |
| `INTERNAL_AUTH_USER` | (Optionnel) utilisateur dashboard | `admin` |
| `INTERNAL_AUTH_PASS_HASH` | Hash bcrypt si activ√© | `$2b$...` |
| `LOG_LEVEL` | Niveau logs | `INFO` |
| `MAX_POSTS_PER_KEYWORD` | Limite extraction par mot-cl√© | `30` |
| `JOB_VISIBILITY_TIMEOUT` | Timeout r√©apparition job (s) | `300` |
| `EXPORT_DIR` | Dossier exports CSV | `exports` |
| `RECRUITMENT_SIGNAL_THRESHOLD` | Seuil compteur m√©trique recrutement (champ non stock√©) | `0.35` |
| `SHUTDOWN_TOKEN` | Jeton requis pour POST `/shutdown` | `secret123` |

---
## üöÄ D√©marrage Local (Windows PowerShell)
```powershell
python -m venv .venv
. .venv\Scripts\Activate.ps1
pip install -r requirements.txt
playwright install chromium
Copy-Item .env.example .env
# √âditer .env avec vos valeurs

# Lancer serveur API seul
uvicorn server.main:app --reload --port 8000

# Lancer worker seul
python scripts/run_worker.py

# Lancer serveur + worker ensemble (d√©mo rapide)
python scripts/run_all.py
```
Acc√©der au dashboard: http://127.0.0.1:8000/

D√©clencher manuellement un job (sans UI) :
```powershell
python scripts\run_once.py --keywords "python;ai"
```

#### Cache Playwright (CI)
Le workflow `release.yml` met en cache les navigateurs via `actions/cache` (cl√© `playwright-browsers-...`).
Invalider le cache apr√®s upgrade Playwright : modifier la cl√© dans le workflow.

---
## üê≥ Docker
Build & run :
```bash
docker build -t linkedin-scraper .
# Cr√©er un r√©seau si usage conteneurs Redis/Mongo
# docker network create internal_net

# Exemple (avec variables inline de test)
docker run --rm -p 8000:8000 --env-file .env linkedin-scraper
```
Le worker peut √™tre un second conteneur (m√™me image) avec commande override :
```bash
docker run --rm --env-file .env linkedin-scraper python -m scraper.worker
```
Pour Playwright dans Docker : Chromium install√© au build + d√©pendances system (voir `Dockerfile`).

---
## üß™ Qualit√© & Tests
Commandes :
```powershell
ruff check .
black --check .
pytest -q --asyncio-mode=auto --maxfail=1 --disable-warnings
mypy .
```
Couverture :
```powershell
pytest --cov=scraper --cov=server --cov-report=term-missing
```
Auto-fix format :
```powershell
ruff check . --fix
black .
```

---
## üìä Observabilit√©
| Aspect | D√©tails |
|--------|---------|
| Logs | JSON via `structlog`, enrichis `request_id`, niveau configurable `LOG_LEVEL` |
| Rotation | Handler `RotatingFileHandler` (variables ci‚Äëdessous) |
| Metrics | Endpoint `/metrics` (Prometheus) exposant compteurs & histogrammes |
| Screenshots | Captur√©s sur √©checs critiques Playwright dans `screenshots/` |
| Traces futures | OpenTelemetry (roadmap) |

### M√©triques expos√©es
| Nom | Type | Description |
|-----|------|-------------|
| `scrape_jobs_total{status=success|error}` | Counter | Nombre de jobs trait√©s par statut |
| `scrape_posts_extracted_total` | Counter | Total de posts extraits (par job) |
| `scrape_duration_seconds` | Histogram | Dur√©e des jobs de scraping |
| `scrape_mock_posts_extracted_total` | Counter | Posts synth√©tiques g√©n√©r√©s (mode mock) |
| `scrape_storage_attempts_total{backend,result}` | Counter | Succ√®s/erreurs par backend (mongo/sqlite/csv) |
| `scrape_queue_depth` | Gauge | Profondeur actuelle de la file de jobs Redis |
| `scrape_job_failures_total` | Counter | Nombre de jobs √©chou√©s (exceptions) |
| `scrape_step_duration_seconds{step}` | Histogram | Dur√©e de sous-√©tapes (mongo_insert, sqlite_insert, etc.) |
| `scrape_rate_limit_wait_seconds_total` | Counter | Secondes cumul√©es d'attente dues au rate limiting |
| `scrape_rate_limit_tokens` | Gauge | Jetons disponibles (bucket courant) |
| `api_rate_limit_rejections_total` | Counter | Requ√™tes API rejet√©es (limitation IP) |
| `scrape_scroll_iterations_total` | Counter | Nombre total d'it√©rations de scroll ex√©cut√©es |
| `scrape_extraction_incomplete_total` | Counter | Extractions arr√™t√©es sous le seuil `MIN_POSTS_TARGET` |
| `scrape_recruitment_posts_total` | Counter | Posts d√©tect√©s recrutement (heuristique interne, score non stock√©) |

Endpoints op√©rationnels additionnels :
| Endpoint | M√©thode | Description |
|----------|---------|-------------|
| `/health` | GET | Sant√© enrichie (ping Mongo, last_run, age, queue_depth, flags) |
| `/shutdown` | POST | Arr√™t contr√¥l√© (token + √©ventuellement basic auth) |
| `/debug/auth` | GET | Diagnostic session Playwright (storage_state, modes) |
| `/debug/last_batch` | GET | Derniers posts (auteur, company, keyword, timestamps) pour debug extraction |
| `/api/stats` | GET | Statistiques runtime agr√©g√©es (mock_mode, intervalle autonome, posts_count, √¢ge last_run, queue_depth) |
| `/api/version` | GET | M√©tadonn√©es build (commit, timestamp) pour tra√ßabilit√© |

### Statistiques suppl√©mentaires (meta)
Le document meta Mongo (`_id: "global"`) contient d√©sormais :
```jsonc
{
  "posts_count": 1234,
  "last_run": "2025-09-19T09:10:11.123456+00:00",
  "last_job_posts": 42,
  "last_job_unknown_authors": 5,
  "last_job_unknown_ratio": 0.119,
  "scraping_enabled": true
}
```
Ces champs apparaissent partiellement dans `/health` : `last_job_unknown_authors`, `last_job_posts`, `last_job_unknown_ratio` pour rapidement suivre la qualit√© de d√©tection auteur.

### Capture d'authentification
Un screenshot `screenshots/auth_state.png` est g√©n√©r√© √† chaque tentative d'initialisation de session (utile si auteurs restent `Unknown`).

### Variables de configuration Logging
| Variable | R√¥le | Exemple |
|----------|------|---------|
| `LOG_FILE` | Active la sortie fichier si d√©fini | `logs/app.log` |
| `LOG_MAX_BYTES` | Taille max d'un fichier avant rotation | `2000000` |
| `LOG_BACKUP_COUNT` | Nombre de fichiers conserv√©s | `5` |

### Contexte de configuration
Les settings utilisent d√©sormais `pydantic-settings` (Pydantic v2) ‚Äî `BaseSettings` ayant √©t√© d√©plac√© hors du core. Le chargement se fait automatiquement depuis `.env` + variables d'environnement.

---
## üîÅ Strat√©gie de Retry (Tenacity)
- Backoff exponentiel + jitter (al√©a contr√¥l√©) pour limiter les patterns d√©tectables
- Nombre de tentatives configurable (`MAX_RETRIES`)
- Erreurs transitoires encapsul√©es (ex: navigation timeouts) pour r√©essai cibl√©
- Extension future : circuit-breaker / compteur d'√©checs cons√©cutifs

---
## üß™ Tests Cl√©s Pr√©vus
| Test | Description |
|------|-------------|
| Selectors snapshot | V√©rifie structure DOM attendue / fallback si changement |
| Storage fallback | Simule indisponibilit√© Mongo ‚áí bascule SQLite/CSV |
| API pagination | V√©rification limites, pages vides |
| Queue job lifecycle | Insert ‚Üí consume ‚Üí ack timeout |
| Lock anti-concurrent | Double lancement worker refus√© |
| Lang detection | Multi-langue texte court/long |

---
## üß¨ Scores supprim√©s
Les champs `score` et `recruitment_score` ont √©t√© retir√©s du mod√®le persistant pour simplifier l'usage m√©tier. La logique de d√©tection recrutement subsiste uniquement comme incr√©ment de m√©trique `scrape_recruitment_posts_total`. Toute donn√©e legacy est migr√©e (SQLite) ou simplement ignor√©e (Mongo d√©j√† sans nouveau champ lors d'insertion). Aucune action manuelle requise.

---
## ‚öñÔ∏è Avertissement L√©gal & √âthique
- Ne pas surcharger LinkedIn (delais random + limites strictes)
- Ne pas republier / revendre le contenu extrait
- D√©sactiver imm√©diatement si modification CGU non compatible
- Stocker minimum de donn√©es n√©cessaires

---
## üó∫Ô∏è Roadmap Potentielle
- Int√©gration OpenTelemetry traces
- Export parquet / Data Lake
- Scheduling cron (APScheduler) au lieu d'appui manuel
- Support login rotation comptes
- D√©tection CAPTCHA & pause adaptative

---
## ü§ù Contributions Internes
1. Cr√©er branche feature
2. Ajouter tests + docs br√®ves
3. Lint & format avant PR
4. Revue par pair interne

---
## üßæ Licence
Usage interne priv√© (pas de distribution publique).

---
## ‚úÖ Statut
MVP fonctionnel livr√© : worker Playwright, stockage multi-niveaux, API & dashboard, m√©triques, logging structur√© + rotation, tests de base. Prochaines √©tapes optionnelles : durcir s√©lecteurs, enrichir CI/CD, ajout d'une strat√©gie anti-CAPTCHA.

---
## üß© Installation (Binaires Desktop / Serveur Local)

Des installateurs sont produits automatiquement √† chaque tag `v*` via GitHub Actions (workflow `build-release`).

### Windows (.msi)
1. T√©l√©charger `LinkedInScraper_<version>.msi` depuis **Releases**.
2. Lancer l'installateur (scope machine par d√©faut).
3. Un raccourci bureau "LinkedInScraper" est cr√©√©.
4. D√©marrer l'application puis ouvrir http://127.0.0.1:8000
5. Premi√®re ex√©cution : t√©l√©chargement √©ventuel de Chromium Playwright (r√©seau requis).

#### Cache Playwright (CI)
Le workflow `release.yml` met en cache les navigateurs via `actions/cache` (cl√© `playwright-browsers-...`).
Invalider le cache apr√®s upgrade Playwright : modifier la cl√© dans le workflow.

### macOS (.dmg)
1. T√©l√©charger `LinkedInScraper_<version>.dmg`.
2. Glisser l'application dans `Applications`.
3. Si Gatekeeper bloque : clic droit ‚Üí Ouvrir.
4. Acc√©der ensuite √† http://127.0.0.1:8000

#### (Optionnel) Signature & Notarisation macOS
Bloc comment√© pr√™t dans `release.yml` : d√©‚Äëcommenter + secrets (`MACOS_CERT_B64`, `MACOS_CERT_PASSWORD`, `MACOS_NOTARY_APPLE_ID`, `MACOS_NOTARY_TEAM_ID`, `MACOS_NOTARY_PASSWORD`) pour activer codesign + notarisation.

### Mises √† jour
Installer simplement la nouvelle version (.msi ou .dmg). Sauvegarder `fallback.sqlite3` si vous utilisez le mode sans Mongo.

### Variables d'environnement
Placer un fichier `.env` √† c√¥t√© de l'ex√©cutable ou d√©finir dans l'environnement syst√®me :
```
MONGO_URI=...
SCRAPE_KEYWORDS=avocat;juriste
LEGAL_DAILY_POST_CAP=50
INTERNAL_AUTH_USER=admin
INTERNAL_AUTH_PASS=ChangeMe!
```

### Stockage local
Sans `MONGO_URI`, un fichier `fallback.sqlite3` est cr√©√© dans le dossier courant.

### D√©sinstallation
Windows : Param√®tres ‚Üí Applications ‚Üí LinkedInScraper ‚Üí D√©sinstaller.
macOS : Supprimer l'app dans Applications + supprimer les artefacts locaux si d√©sir√©.

### G√©n√©ration locale rapide
Windows :
```powershell
pwsh scripts/packaging/build_installer_windows.ps1 -Version 1.2.3
```
macOS :
```bash
VERSION=1.2.3 bash scripts/packaging/macos/build_dmg.sh
```
Le binaire combine serveur + worker via un ¬´¬†entrypoint¬†¬ª unifi√© (`entrypoint.py`) qui d√©marre simultan√©ment le serveur FastAPI et le worker et respawne le worker en cas de crash (cooldown 300s configurable via `WORKER_RESPAWN_COOLDOWN_SECONDS`).

### Entrypoint Unifi√© & Mode Test
Orchestre : serveur FastAPI + worker supervis√© + chargement `.env` + rotation logs.
Mode test rapide (utilis√© dans la suite Pytest) :
```powershell
ENTRYPOINT_TEST_MODE=1 python entrypoint.py
```

---
## üåê D√©ploiement Gratuit / Low-Cost

Priorit√©: Deta Space (gratuit), sinon Render (Free plan) ou Railway (Free trial / low-cost). Le scraping r√©el continu avec Playwright n√©cessite un runtime supportant Chromium (Deta Space n'ex√©cute pas de navigateur complet de fa√ßon fiable) ‚áí mode mock recommand√© sur Deta Space.

### Variables Cl√©s de D√©ploiement
| Variable | R√¥le |
|----------|------|
| `PLAYWRIGHT_MOCK_MODE` | `1` pour donn√©es synth√©tiques (CI / Deta) ; `0` pour vrai scraping |
| `AUTONOMOUS_WORKER_INTERVAL_SECONDS` | Intervalle secondes entre cycles auto (ex: 900) |
| `INPROCESS_AUTONOMOUS` | `1` pour ex√©cuter le worker dans le m√™me process FastAPI (utile Deta) |
| `DASHBOARD_PUBLIC` | `1` rendu public, sinon activer auth interne |
| `MONGO_URI` | Connexion MongoDB Atlas (persistance) sinon fallback SQLite |
| `WORKER_RESTART_DELAY_SECONDS` | D√©lai red√©marrage worker d√©di√© (Render/Railway) |
| `PORT` | Port impos√© par la plateforme (Render/Railway) |
| `INTERNAL_AUTH_USER` | Active Basic Auth si d√©fini (toujours appliqu√© m√™me avec `DASHBOARD_PUBLIC=1`) |
| `INTERNAL_AUTH_PASS_HASH` | Hash bcrypt explicite si d√©j√† g√©n√©r√© |
| `INTERNAL_AUTH_PASS` | Mot de passe en clair (hash g√©n√©r√© automatiquement si HASH absent) |
| `STORAGE_STATE_B64` | Contenu base64 de `storage_state.json` inject√© au d√©marrage si fichier manquant |

### 1. Deta Space (Mock Mode Conseill√©)
1. Installer l'outil Deta & login.
2. Ajouter le `Spacefile` fourni √† la racine (d√©j√† pr√©sent).
3. D√©ployer: `deta space push`.
4. Dans l'interface Space, ajouter les variables d'environnement souhait√©es (ex: `PLAYWRIGHT_MOCK_MODE=1`, `INPROCESS_AUTONOMOUS=1`, `AUTONOMOUS_WORKER_INTERVAL_SECONDS=900`).
5. (Optionnel) Ajouter `MONGO_URI` vers un cluster Atlas pour persistance; sinon les donn√©es seront dans `fallback.sqlite3` interne (√©ph√©m√®re sur rebuilds).

Limitations Deta:
- Pas de navigateur Chrome complet stable ‚áí mode r√©el non garanti.
- Utiliser le mode mock pour d√©monstration du dashboard + SSE.

### 2. Render (Web + Worker s√©par√©s)
Fichiers utilis√©s: `render.yaml`, `Procfile`..
1. Cr√©er un nouveau Blueprint dans Render √† partir du repo (connect GitHub).
2. Render d√©tecte `render.yaml` et provisionne deux services :
  - Web: lance `python scripts/run_server.py` sur le port `$PORT`.
  - Worker: lance `python scripts/run_worker.py` avec red√©marrage automatique.
3. Dans l'onglet Environment, ajouter (exemple r√©el minimal):
  - `MONGO_URI=...` (Atlas)
  - `PLAYWRIGHT_MOCK_MODE=0`
  - `STORAGE_STATE_B64=<base64 du storage_state.json>` (ou montez le fichier via volume priv√©)
  - `INTERNAL_AUTH_USER=admin` + `INTERNAL_AUTH_PASS=ChangeMe!` (hash auto)
  - `AUTONOMOUS_WORKER_INTERVAL_SECONDS=0` (si worker d√©di√© s√©par√©) ou >0 si vous supprimez le service worker.
  - (Optionnel) `DASHBOARD_PUBLIC=1` pour acc√®s sans auth si aucune variable INTERNAL_AUTH_*.
4. (Optionnel) Ajouter un Redis manag√©. Sinon le worker autonome tournant p√©riodiquement suffit.
5. Fournir la session: soit via `STORAGE_STATE_B64`, soit en attachant apr√®s d√©ploiement un fichier `storage_state.json`. Le bootstrap d√©codera automatiquement la variable si le fichier est absent.

SSE: Render supporte les connexions persistantes HTTP/1.1 ‚áí `/stream` fonctionne.

### 3. Railway
1. Rails nouveau projet ‚Üí connecter repository.
2. Ajouter deux services manuels si souhait√©: `web` (FastAPI) & `worker` (m√™me image, commande diff√©rente) ou un seul service avec `INPROCESS_AUTONOMOUS=1`.
3. Dans Variables, d√©finir valeurs analogues √† Render.
4. S'assurer d'installer Playwright dans postinstall (ex: `nixpacks` build d√©tecte requirements puis ajouter hook: `python -m playwright install --with-deps chromium`).

### 4. Docker Compose (Auto- h√©bergement VPS)
Utiliser `docker-compose.yml` existant: un service API + un worker + Redis + Mongo si souhait√©. Adapter `.env`.

### 5. Authentification & Public
- D√©mo publique: `DASHBOARD_PUBLIC=1`, laisser `INTERNAL_AUTH_USER` vide.
- Production interne: `DASHBOARD_PUBLIC=0` puis d√©finir `INTERNAL_AUTH_USER` + `INTERNAL_AUTH_PASS_HASH`.


### 6. Fournir `storage_state.json`
Scraping r√©el LinkedIn n√©cessite une session authentifi√©e:
1. En local: lancer `playwright codegen https://www.linkedin.com/feed/` ou navigation manuelle via script pour login.
2. Exporter storage state: adapter un petit script Playwright pour sauvegarder `storage_state.json`.
3. Ne jamais committer ce fichier. Le fournir √† l'environnement (ex: Render) via un secret base64:
  - Encoder (PowerShell): `Set-Content -Path storage_state.b64 -Value ([Convert]::ToBase64String([IO.File]::ReadAllBytes('storage_state.json')))`
  - Encoder (Linux/macOS): `base64 -w0 storage_state.json > storage_state.b64`
  - Variable: `STORAGE_STATE_B64=<contenu du fichier .b64>`
  - Le bootstrap d√©code automatiquement si `storage_state.json` est absent.

### 6bis. Basic Auth l√©g√®re m√™me en mode public
Si `DASHBOARD_PUBLIC=1` mais que vous d√©finissez `INTERNAL_AUTH_USER` + ( `INTERNAL_AUTH_PASS_HASH` ou `INTERNAL_AUTH_PASS` ):
- CORS large activ√© (acc√®s JS depuis ailleurs) mais endpoints prot√©g√©s par Basic Auth.
- Utiliser un header: `Authorization: Basic base64(user:password)`.
- Pour √©viter stocker un hash manuellement: mettre `INTERNAL_AUTH_PASS=monmotdepasse` et laisser vide `INTERNAL_AUTH_PASS_HASH`.


### 7. Mode Autonome In-Process
Activez `INPROCESS_AUTONOMOUS=1` et `AUTONOMOUS_WORKER_INTERVAL_SECONDS>0`. Le serveur FastAPI d√©marrera une t√¢che asynchrone de scraping p√©riodique (m√™me logique que le worker) ‚Äî utile quand la plateforme ne permet pas de process s√©par√©.

### 8. Export CSV Minimal
```
python scripts/export_csv.py --out exports/posts_snapshot.csv --limit 1000
```
Colonne: `_id, keyword, author, company, text, language, published_at, collected_at, permalink`.

### 9. Surveillance / Fiabilit√©
- Red√©marrage worker automatique (script `run_worker.py`) ‚Üí log en stdout sur crash.
- M√©triques Prometheus (`/metrics`) : v√©rifier `scrape_jobs_total`, `scrape_post_extracted_total`, `scrape_recruitment_posts_total`.
- SSE temps r√©el: navigateur √©coute `/stream` (√©v√©nements `job_complete`, `toggle`).

### 10. Plans d'√©volution Cloud
| Besoin | Option |
|--------|--------|
| Multi-instance / scaling | Redis externe pour queue + cache rate limit |
| Observabilit√© avanc√©e | Ajouter OpenTelemetry + exporter traces |
| Persist de session Playwright | Stocker storage_state chiffr√© (KMS) |
| Anti blocage | Proxy rotatif / user-agents dynamiques |

---

### Note Playwright (r√©seau / proxy interne)
Si l'installation du navigateur √©choue avec une erreur de certificat (`SELF_SIGNED_CERT_IN_CHAIN`) :
1. V√©rifier le proxy d'entreprise (ex: config syst√®me / variables `HTTP_PROXY` / `HTTPS_PROXY`).
2. Ajouter le certificat racine interne dans le magasin syst√®me.
3. En dernier recours (non recommand√© long terme) :
  ```powershell
  setx NODE_TLS_REJECT_UNAUTHORIZED 0
  # puis dans un nouveau terminal
  python -m playwright install chromium
  ```
4. Pour CI sans navigateur : utiliser `PLAYWRIGHT_SKIP_BROWSER_DOWNLOAD=1` puis installer au runtime contr√¥l√©.

Retirer la d√©sactivation TLS aussit√¥t que la cha√Æne de confiance est corrig√©e.

---
## üéØ D√©tection Signal Recrutement (Nouveaut√©)
Objectif : identifier les posts susceptibles d'√™tre des signaux de recrutement (annonce explicite, sourcing, besoins √©quipe, ouverture de poste) dans les domaines juridiques / fiscaux / data / tech.

### Heuristique
La fonction `compute_recruitment_signal(text)` applique :
1. Normalisation (lowercase, accents retir√©s, ponctuation simplifi√©e)
2. Tokenisation + stemming tr√®s l√©ger (suffixes fran√ßais usuels)
3. Pond√©ration :
   - Mots/lemmes indicateurs (ex: `recrut`, `poste`, `hiring`, `rejoindre`, `talent`, `cdi`, `alternance`) ‚áí poids individuel
   - Phrases cl√©s (bigrammes / trigrammes) comme `nous recrutons`, `on recrute`, `offre d emploi`, `recherche son/sa`, `hiring for`, `join our team` ‚áí bonus suppl√©mentaire
4. Score brut liss√© et clamp√© dans [0,1] (log / normalisation longueur pour √©viter sur-pond√©ration de r√©p√©titions).

### Seuil & M√©trique
- Le seuil configur√© via `RECRUITMENT_SIGNAL_THRESHOLD` (ex: 0.35) d√©termine l'incr√©ment de la m√©trique `scrape_recruitment_posts_total`.
- Tous les posts stockent de toute fa√ßon `recruitment_score` (nullable si ancienne donn√©e ou mode legacy).

### Filtrage Dashboard & API
- Dashboard : champ num√©rique "Score recrutement ‚â•" (query param `min_score`).
- API `/api/posts?min_score=0.4` renvoie uniquement les posts dont `recruitment_score` ‚â• valeur.
  - Si `min_score` absent ‚áí pas de filtrage.

### Stockage & Compatibilit√©
- Mongo : champ `recruitment_score` ajout√© dans chaque document (nullable).
- SQLite : colonne ajout√©e automatiquement si base cr√©√©e apr√®s la fonctionnalit√©; pour une base existante ex√©cuter :
  ```sql
  ALTER TABLE posts ADD COLUMN recruitment_score REAL;
  ```
  (Optionnel : laisser NULL pour historiques.)
- CSV : nouvelle colonne `recruitment_score` apr√®s `score`.

### Tests
`tests/test_recruitment_scoring.py` couvre :
- Bas niveau sur texte neutre (score faible)
- Texte riche en signaux (score √©lev√©)
- Stabilit√© du domaine [0,1]

### Ajustements Futurs (Id√©es)
- Pond√©ration contextuelle (ex: pr√©sence d'un lien vers une offre)
- D√©tection langue + mapping lexiques multi-langues
- Mod√®le ML l√©ger (TF-IDF / logreg) si corpus √©tiquet√© interne disponible
- D√©corr√©lation bruit marketing vs. v√©ritables annonces via pattern n√©gatifs

---
## üîÑ Fallback Storage Test√©
Un test (`tests/test_fallback_storage.py`) v√©rifie :
1. Insertion SQLite quand Mongo absent.
2. Fallback CSV forc√© en simulant une erreur SQLite.

---
## üß™ Configuration Lint & Type
Fichiers ajout√©s : `ruff.toml`, `mypy.ini` pour coh√©rence multi-environnements.

---
## ‚öôÔ∏è Nouveaut√©s Techniques (v1.2.0+)
- **Champs quotidiens juridiques** : suivi des posts accept√©s et rejet√©s par intent/location + quota journalier.
- **Rate limiting** : protection basique par IP (en m√©moire) + seau de jetons (token bucket) pour limiter le scraping excessif.
- **Scrolling am√©lior√©** : extraction progressive des r√©sultats avec d√©tection de compl√©tude.
- **Signal de recrutement** : d√©tection heuristique des posts √† potentiel de recrutement dans les domaines cibles.
- **Fallback storage** : m√©canisme de secours testable pour MongoDB ‚Üí SQLite ‚Üí CSV.
- **Tests & CI** : couverture accrue des tests, int√©gration continue avec GitHub Actions.

### Champs Quotidiens (Quota Juridique)
Suivi runtime (r√©initialis√© √† minuit UTC):
| Champ | Description |
|-------|-------------|
| `legal_daily_date` | Date UTC suivie |
| `legal_daily_count` | Posts accept√©s dans la journ√©e |
| `legal_daily_discard_intent` | Rejets (intent) |
| `legal_daily_discard_location` | Rejets (location) |
Expos√© via `/api/legal_stats`.

### Rate Limiting (IP In-Memory)
Param√®tres :
```
API_RATE_LIMIT_PER_MIN=60
API_RATE_LIMIT_BURST=20
```
Limitation de base par IP (LRU ~512 IP). √Ä distribuer via Redis pour d√©ploiements multi-instances. M√©trique de rejet: `api_rate_limit_rejections_total`.

### Token Bucket (Rate Limit R√©el)
### Scrolling & Compl√©tude (Nouveaut√©s)
Nouveaux param√®tres pour affiner l'extraction progressive des r√©sultats paresseusement charg√©s :
```
MAX_SCROLL_STEPS=5      # Limite dure d'it√©rations de scroll suppl√©mentaires
SCROLL_WAIT_MS=1200     # Attente (ms) apr√®s chaque scroll pour laisser charger le DOM
MIN_POSTS_TARGET=10     # Seuil minimal de posts avant d'accepter un arr√™t anticip√©
```
Logique d'arr√™t :
1. Posts >= `MAX_POSTS_PER_KEYWORD` ‚áí stop
2. Posts >= `MIN_POSTS_TARGET` ET aucune augmentation apr√®s une it√©ration ‚áí stop
3. `MAX_SCROLL_STEPS` atteint ‚áí stop (marqu√© incomplete si < seuil)

M√©triques associ√©es :
- `scrape_scroll_iterations_total` : incr√©ment√©e √† chaque scroll tent√©
- `scrape_extraction_incomplete_total` : incr√©ment si extraction < `MIN_POSTS_TARGET` en fin de boucle

Objectif : instrumenter la ¬´ profondeur ¬ª requise pour atteindre la compl√©tude et calibrer les valeurs par environnement (CI vs prod restreinte).

Param√®tres :
```
RATE_LIMIT_BUCKET_SIZE=120      # Capacit√© maximale (burst autoris√©)
RATE_LIMIT_REFILL_PER_SEC=2.0   # D√©bit de r√©g√©n√©ration
```

---
## üîß Classifier Tuning (Legal Intent)
Le classifieur heuristique privil√©gie la pr√©cision (faible taux de faux positifs). Pour augmenter le rappel :

### Param√®tres rapides
| Levier | Effet | Recommandation |
|--------|-------|----------------|
| `LEGAL_INTENT_THRESHOLD` | Abaisse le seuil d'acceptation | Descendre par paliers de 0.05 (min conseill√© 0.20) |
| `FILTER_LEGAL_DOMAIN_ONLY` | Pr√©-filtre hors domaine | D√©sactiver (`0`) si couverture multi-domaine souhait√©e |
| `RECRUITMENT_SIGNAL_THRESHOLD` | Influence posts retenus avant classification (filtre opportunit√©s) | Synchroniser avec intention si trop strict |

### Strat√©gies d'am√©lioration rappel
1. Ajouter des expressions explicites dans les phrases de recrutement (liste `RECRUITMENT_PHRASES`) ‚Äì pull request cibl√©e.
2. √âtendre `LEGAL_ROLE_KEYWORDS` pour nouveaux intitul√©s rares (ex: "compliance officer", "juriste propri√©t√© intellectuelle").
3. Introduire un mode "exploratoire" : activer une variable `LEGAL_EXPLORATORY_MODE=1` (√† impl√©menter) qui :
   - Abaisse le seuil de 0.05 automatiquement sur les 10 premiers posts rejet√©s.
   - Loggue chaque acceptation marginale avec un tag `exploratory_accept`.
4. Collecter un corpus √©tiquet√© interne (‚â•300 exemples) pour calibrer un mod√®le l√©ger (logreg TF-IDF) ‚Äì future optional.

### Indicateurs √† surveiller
| M√©trique | Interpr√©tation | Action si anomalie |
|----------|----------------|--------------------|
| `legal_posts_discarded_total{reason="intent"}` / `legal_intent_classifications_total` | Taux de rejet intent √©lev√© | V√©rifier faux n√©gatifs, ajuster seuil |
| `legal_daily_cap_reached_total` | Cap atteint t√¥t dans la journ√©e | Augmenter cap ou restreindre mots-cl√©s |
| `rejection_rate` (API stats) > 0.7 | Classifieur trop strict | Ajouter phrases / baisser seuil |

### Proc√©dure de tuning contr√¥l√©
1. Baisser `LEGAL_INTENT_THRESHOLD` de 0.35 ‚Üí 0.30 sur un run test (mock ou dataset captur√©).
2. Comparer : nombre d'acceptations + spot-check manuel (√©chantillon 20 posts nouveaux).
3. Si <10% d'acceptations semblent des faux positifs, conserver ; sinon revenir et enrichir phrases cl√©s.
4. Documenter la d√©cision dans `CHANGELOG.md`.

### Future Idea: Adaptive Threshold
Pseudocode (non impl√©ment√©) :
```python
if last_50_decisions and rejection_rate_last_50 > 0.85:
    threshold = max(base_threshold - 0.05, 0.20)
else:
    threshold = base_threshold
```
Avantage: augmente rappel lors de phases de sous-couverture sans rel√¢cher durablement la pr√©cision.

---

