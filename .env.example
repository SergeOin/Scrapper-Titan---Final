# ==============================================
# Environment Variables Example
# Copy this file to .env and adjust values.
# NEVER commit your real .env.
# ==============================================

# -------- Core Application --------
APP_NAME=linkedin-scraper
LOG_LEVEL=INFO
LOG_FILE=logs/app.json.log            # Optional: enable file logging (comment to disable)
LOG_MAX_BYTES=2000000                 # Rotation max size bytes (~2MB)
LOG_BACKUP_COUNT=5                    # Number of rotated files to keep
SCRAPING_ENABLED=1               # 1 = enabled, 0 = disable all scraping activity
PLAYWRIGHT_HEADLESS=1            # 1 = headless browser, 0 = visible for debug
PLAYWRIGHT_MOCK_MODE=0           # 1 = génère des posts synthétiques (sans navigateur) pour tests
MAX_MOCK_POSTS=5                 # Limite de posts synthétiques par mot-clé en mode mock
AUTONOMOUS_WORKER_INTERVAL_SECONDS=900  # Si >0 le worker autonome périodique s'exécute (mock ou réel)
INPROCESS_AUTONOMOUS=0           # 1 = lancer le worker périodique DANS le serveur FastAPI (ex: Deta Space)
WORKER_RESTART_DELAY_SECONDS=5   # Délai avant relance en cas de crash du worker dédié
DASHBOARD_PUBLIC=1               # 1 = CORS large / pas d'auth interne (mettre 0 + activer INTERNAL_AUTH_* en prod)
CONCURRENCY_LIMIT=2              # Nombre maximum de jobs traités en parallèle (sémaphore)
PER_KEYWORD_DELAY_MS=500         # Délai ajouté entre deux mots-clés dans un même job
GLOBAL_RATE_LIMIT_PER_MIN=120    # Limite douce globale (placeholder token bucket)
RATE_LIMIT_BUCKET_SIZE=120       # Capacité max du bucket de jetons (hard ceiling)
RATE_LIMIT_REFILL_PER_SEC=2.0    # Jetons régénérés par seconde
MAX_POSTS_PER_KEYWORD=30         # Hard cap per keyword per run
MAX_SCROLL_STEPS=5               # Nombre max de scrolls supplémentaires par mot-clé
SCROLL_WAIT_MS=1200              # Attente après chaque scroll (ms)
MIN_POSTS_TARGET=10              # Objectif minimum avant d'arrêter si plus de croissance
CACHE_TTL_SECONDS=300            # In-memory TTL for lightweight caches (seconds)
LOCK_FILE=.scrape.lock           # File-based lock to avoid concurrent workers
EXPORT_DIR=exports               # Directory for CSV fallback exports

# -------- Keywords (semicolon separated) --------
SCRAPE_KEYWORDS="Avocat collaborateur;Avocat associé;Avocat Counsel;Paralegal;Legal counsel;Juriste;Responsable juridique;Directeur Juridique;Notaire stagiaire;Notaire associé;Notaire salarié;Notaire assistant;Clerc de notaire;Rédacteur d'actes;Responsable fiscal;Directeur fiscal;Comptable taxateur;Formaliste"

# -------- MongoDB (Primary Storage) --------
MONGO_URI=mongodb://localhost:27017
MONGO_DB=linkedin_scrape
MONGO_COLLECTION_POSTS=posts
MONGO_COLLECTION_META=meta
MONGO_CONNECT_TIMEOUT_MS=5000

# -------- Redis (Queue + Cache) --------
REDIS_URL=redis://localhost:6379/0
REDIS_QUEUE_KEY=jobs:scrape
JOB_VISIBILITY_TIMEOUT=300        # Seconds before an unacked job is re-queued
JOB_POLL_INTERVAL=3               # Seconds between polling attempts when idle

# -------- SQLite Fallback --------
SQLITE_PATH=fallback.sqlite3

# -------- CSV Fallback (tertiary) --------
CSV_FALLBACK_FILE=exports/fallback_posts.csv

# -------- Internal Auth (Optional) --------
# To enable, set INTERNAL_AUTH_USER and INTERNAL_AUTH_PASS_HASH (bcrypt hash)
INTERNAL_AUTH_USER=
INTERNAL_AUTH_PASS_HASH=
# Option simplifiée : mot de passe en clair (hash bcrypt généré au démarrage si PASS_HASH vide)
INTERNAL_AUTH_PASS=
# NOTE: Le runtime génère automatiquement un hash bcrypt si INTERNAL_AUTH_PASS est défini.
# Version bcrypt épinglée (3.2.2) pour compatibilité passlib 1.7.x. Ne pas mettre à jour vers 4.x sans test.
# Generate hash (example):
# python -c "from passlib.hash import bcrypt; print(bcrypt.hash('ChangeMe!'))"

# -------- Security / Rate Limiting (Basic Heuristics) --------
MIN_SLEEP_MS=900                  # Min sleep between page actions
MAX_SLEEP_MS=2500                 # Max sleep between page actions
RANDOM_UA_ROTATION=1              # 1 = rotate realistic User-Agent per run

# -------- Metrics / Monitoring --------
ENABLE_METRICS=1                  # Expose /metrics endpoint

# -------- Timeouts / Retries --------
NAVIGATION_TIMEOUT_MS=15000
ELEMENT_TIMEOUT_MS=8000
MAX_RETRIES=4

# -------- Files / Artifacts --------
SCREENSHOT_DIR=screenshots
TRACE_DIR=traces
STORAGE_STATE=storage_state.json  # Playwright authenticated state file
LOGIN_INITIAL_WAIT_SECONDS=0      # Fenêtre (s) pour login manuel première navigation si non authentifié
STORAGE_STATE_B64=                # Variante base64 du contenu JSON de storage_state pour déploiement PaaS
# Si défini et que STORAGE_STATE (fichier) est absent, il sera décodé automatiquement au démarrage.

# -------- Scoring Heuristic Weights (0..1) --------
WEIGHT_LENGTH=0.4
WEIGHT_MEDIA=0.3
WEIGHT_KEYWORD_DENSITY=0.2
WEIGHT_LANG_MATCH=0.1

# -------- Language / Locale --------
DEFAULT_LANG=fr

# -------- Advanced (Usually Leave As Default) --------
HTTPX_TIMEOUT=15
DISABLE_SSL_VERIFY=0
TRIGGER_TOKEN=                 # Jeton partagé requis pour /trigger si défini
API_RATE_LIMIT_PER_MIN=60      # Requêtes par minute par IP (approx)
API_RATE_LIMIT_BURST=20        # Burst initial autorisé
RECRUITMENT_SIGNAL_THRESHOLD=0.35 # Seuil métrique scrape_recruitment_posts_total

# -------- Deployment (PaaS) --------
PORT=8000                      # Certains PaaS (Render/Railway) injectent PORT automatiquement
APP_HOST=0.0.0.0
APP_COMMIT=                    # (Optionnel) Injecté au build/deploy (git rev) pour /api/version
BUILD_TIMESTAMP=               # (Optionnel) Timestamp ISO UTC du build pour /api/version

# -------- Future Expansion (placeholders) --------
# OPENTELEMETRY_EXPORTER=
# SENTRY_DSN=

# ==============================================
# End of file
# ==============================================
